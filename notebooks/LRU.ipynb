{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92fce68b",
   "metadata": {},
   "source": [
    "# Logistic Regression Unit\n",
    "- Forms neuron in NN\n",
    "- Find optimal W,b\n",
    "- Y = WTX + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "18aa427a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "class Perceptron:\n",
    "    \"\"\"\n",
    "    Single Layer Perceptron\n",
    "        - Parameters W and b\n",
    "        - Multiple Activation functions\n",
    "        - Gradient Descent capabilities (require step size selection)\n",
    "    \"\"\"\n",
    "    def __init__(self, params: int, step_size = 0.5) -> None: # Add reshaping inputs from ndArrays to 1d\n",
    "        self.W = np.ones(params) # Initial setting\n",
    "        self.b = np.ones(1)\n",
    "        self.step_size = step_size\n",
    "\n",
    "    def run(self, x: np.array, y: np.array) -> None: # Compute forward pass then backwards\n",
    "        forward: float = self._forward(x) # wTx + b\n",
    "        prediction: float = self._sigmoid(forward) # Activation\n",
    "        # error: float = self._loss(prediction, y) # Mean Squared Error -> Not needed for backpropagation\n",
    "        grad: float = self._dLoss(prediction, y)*self._dSigmoid(prediction)\n",
    "        dW: np.array = grad * x\n",
    "        db: np.array = grad\n",
    "        self._gradDescent(dW, db)\n",
    "\n",
    "    def _forward(self, inputs: np.array) -> np.float64: # Forward pass with given inputs\n",
    "        agg = (np.dot(self.W, inputs) + self.b)\n",
    "        return agg       \n",
    "\n",
    "    def _sigmoid(self, x: float) -> float: # Activation function of wTX + b\n",
    "        x = np.clip(x, -500, 500)\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    def _dSigmoid(self, prediction: float) -> float: # Derivative of sigmoid\n",
    "        return prediction*(1-prediction)\n",
    "    \n",
    "    def _dLoss(self, x, y) -> float:\n",
    "        return 2*(x - y)\n",
    "    \n",
    "    def _gradDescent(self, dW: np.array, db: np.array) -> None: # Backpropagation of W and b\n",
    "        self.W = self.W - self.step_size * dW\n",
    "        self.b = self.b - self.step_size * db\n",
    "        # self.step_size /= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d214a3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled W and b\n",
      "[ 1.00000000e+00  6.44608557e+00 -9.55565747e+00 -3.14054928e-03\n",
      "  2.12340091e+00]\n",
      "[-6.21342665]\n"
     ]
    }
   ],
   "source": [
    "perceptron = Perceptron(5, 10)\n",
    "import random\n",
    "for _ in range(9999999):\n",
    "    x1 = random.uniform(-10, 10)\n",
    "    x2 = random.uniform(-10, 10)\n",
    "    x3 = random.uniform(-10, 10)\n",
    "    x4 = random.uniform(-10, 10)\n",
    "    x5 = random.uniform(-10, 10)\n",
    "    y = 1 if x1 + 6*x2 - 9*x3 + 0*x4 + 2*x5 - 6 > 0 else 0 # Replicate W = [1,6,-9,0,2] and b = -6\n",
    "    perceptron.run(np.array([x1, x2, x3, x4, x5]), y)\n",
    "    # print(perceptron.W)\n",
    "    W = perceptron.W\n",
    "    b = perceptron.b\n",
    "\n",
    "\n",
    "scalar = 1/W[0]\n",
    "W = scalar*W\n",
    "b = scalar*b\n",
    "print(\"Scaled W and b\")\n",
    "print(W)\n",
    "print(b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
